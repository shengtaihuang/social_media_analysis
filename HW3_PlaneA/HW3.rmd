---
title: "第九組：TF-IDF 與 Bigram 情緒分析（隻狼：暗影雙死）"
author: "第九組"
date: "2019/04/09"
output:
  html_document: default
html_notebook: default
pdf_document: default
abstract: TF-IDF 與 Bigram 情緒分析
---

```{r}
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # 避免中文亂碼
```

## 安裝需要的packages
```{r echo = F, results = 'hide'}
packages = c("readr", "dplyr", "stringr", "jiebaR", "tidytext", "NLP", "readr", "tidyr", "ggplot2", "ggraph", "igraph", "scales", "reshape2", "widyr")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
```

```{r echo = F, results = 'hide'}
require(readr)
require(dplyr)
require(stringr)
require(jiebaR)
require(tidytext)
require(NLP)
require(tidyr)
require(ggplot2)
require(ggraph)
require(igraph)
require(scales)
require(reshape2)
require(widyr)
require(knitr)
library(kableExtra)
library(data.table)
```
#載入資料集
* 列出前資料前 10 列
```{r}
#載入原始資料集
sekiro<- read_csv("./sekiro_test_articleMetaData.csv") %>%
              mutate(sentence=gsub("[\n]{2,}", "。", sentence))
sekiro %>% head(10) %>% kable %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>% 
  scroll_box(height = "300px")
```

## 對全部的文章進行斷句，並儲存結果
* 將前 10 列斷句後的資料
```{r}
# 以全形或半形 驚歎號、問號、分號 以及 全形句號 爲依據進行斷句
sekiro_sentences <- strsplit(sekiro$sentence,"[。！；？!?;]")
# 將每句句子，與他所屬的文章連結配對起來，整理成一個dataframe
sekiro_sentences <- data.frame(
                        artUrl = rep(sekiro$artUrl, sapply(sekiro_sentences, length)), 
                        sentence = unlist(sekiro_sentences)
                      ) %>%
                      filter(!str_detect(sentence, regex("^(\t|\n| )*$")))

sekiro_sentences$sentence <- as.character(sekiro_sentences$sentence)

sekiro_sentences %>% head(10) %>% kable%>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>% 
  scroll_box(height = "300px")
```


## 接着做斷詞
* 初始化斷詞器
* 斷詞與整理斷詞結果
* 將前出現頻率前 10 大字列出
```{r}
# 使用默認參數初始化一個斷詞引擎
# 先不使用任何的字典和停用詞
jieba_tokenizer = worker()

chi_tokenizer <- function(t) {
  lapply(t, function(x) {
    if(nchar(x)>1){
    tokens <- segment(x, jieba_tokenizer)
    # 去掉字串長度爲1的詞彙
    tokens <- tokens[nchar(tokens)>1]
    return(tokens)
    }
  })
}

# 進行斷詞，並計算各詞彙在各文章中出現的次數
sekiro_words <- sekiro_sentences %>%
  unnest_tokens(word, sentence, token=chi_tokenizer) %>%
  filter(!str_detect(word, regex("[0-9a-zA-Z]"))) %>%
  count(artUrl, word, sort = TRUE)
sekiro_words %>% head(10) 
```




## 計算 tf-idf
* 以每篇文章爲單位，計算每個詞彙在的tf-idf值
```{r}
sekiro_words_tf_idf <- sekiro_words %>%
  bind_tf_idf(word, artUrl, n)

sekiro_words_tf_idf %>% 
  group_by(artUrl) %>%
  top_n(10) %>%
  arrange(desc(artUrl))
```

* 選每篇文章，tf-idf最大的十個詞，
* 並查看每個詞被選中的次數
```{r}

sekiro_words_tf_idf %>% 
  group_by(artUrl) %>%
  top_n(10) %>%
  arrange(desc(artUrl)) %>%
  ungroup() %>%
  count(word, sort=TRUE)
```

> 因爲我們是以每篇文章爲一個document單位（總共有76個document）<br>
  因此我們就不畫課本第三章中，比較各document中tf-idf較高的詞彙比較圖
  
### bigram function
* 載入 stop word 和 negation word 
* 刪除 stop word 後列出前10 大頻率的 bigram
```{r}
# remove stopwords
jieba_tokenizer = worker()

# unnest_tokens 使用的bigram分詞函數
# Input: a character vector
# Output: a list of character vectors of the same length
jieba_bigram <- function(t) {
  lapply(t, function(x) {
    tokens <- segment(x, jieba_tokenizer)
    bigram<- ngrams(tokens, 2)
    bigram <- lapply(bigram, paste, collapse = " ")
    unlist(bigram)
  })
}

# load stop words
stop_words <- scan(file = "./dict/stop_words.txt", what=character(),sep='\n', 
                   encoding='utf-8',fileEncoding='utf-8')
# load negation words
negation_words <- scan(file = "./dict/negation_words.txt", what=character(),sep=',', 
                   encoding='utf-8',fileEncoding='utf-8')

# remove negation words from stop words list
stop_words <- stop_words[!(stop_words %in% c(negation_words))] #如果negation出現在stop word時，不能被刪除

# 執行bigram分詞
sekiro_bigram <- sekiro %>%
  unnest_tokens(bigram, sentence, token = jieba_bigram)


# 清楚包含英文或數字的bigram組合
# 計算每個組合出現的次數
# remove the stop words in bigram
sekiro_bigram %>%
  filter(!str_detect(bigram, regex("[0-9a-zA-Z]"))) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!(word1 %in% stop_words), !(word2 %in% stop_words)) %>%
  count(word1, word2, sort = TRUE) %>%
  unite_("bigram", c("word1","word2"), sep=" ") %>% head(10)
```


* 刪除 stop word 後列出前10 大頻率的 trigram
```{r}
jieba_ngram_3 <- function(t) {
  lapply(t, function(x) {
    tokens <- segment(x, jieba_tokenizer)
    ngram<- ngrams(unlist(tokens), 3)
    ngram <- lapply(ngram, paste, collapse = " ")
    unlist(ngram)
  })
}

# 執行ngram分詞
sekiro_ngram_3 <- sekiro %>%
  unnest_tokens(ngrams, sentence, token = jieba_ngram_3)
sekiro_ngram_3 %>%
  filter(!str_detect(ngrams, regex("[0-9a-zA-Z]"))) %>%
  count(ngrams, sort = TRUE)

# remove the stop words in ngram(n=3)
sekiro_ngram_3 %>%
  filter(!str_detect(ngrams, regex("[0-9a-zA-Z]"))) %>%
  separate(ngrams, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!(word1 %in% stop_words), !(word2 %in% stop_words), !(word3 %in% stop_words)) %>%
  count(word1, word2, word3, sort = TRUE) %>%
  unite_("ngrams", c("word1", "word2", "word3"), sep=" ")%>% head(10)
```
> 從上面的bigram和ngram(n=3)的結果中，我們可以整理出一個斷詞字典。<br>
  我們將詞彙整理好存在dict文件夾中的 sekiro_lexicon.txt 中


### 載入各種字典
* 讀入整理好的字典和情緒字典
```{r}
# load sekiro_lexicon
sekiro_lexicon <- fread(file = "./dict/sekiro_lexicon.txt", header= F,sep='\n', encoding = "UTF-8")
colnames(sekiro_lexicon)[1] <- "word"
sekiro_lexicon <- as.character(sekiro_lexicon$word)
# load game company positive words
game_co_pos_words <- fread(file = "./dict/game_company_positive.txt",sep=',', header= F, stringsAsFactors = F, encoding='UTF-8')
game_co_pos_words <- data.frame(word=t(game_co_pos_words))
rownames(game_co_pos_words)<-c(1:103)
game_co_pos_words <- as.character(game_co_pos_words$word)
# load  game company negation words
game_co_neg_words <- fread(file = "./dict/game_company_negative.txt", header= F, sep=',', stringsAsFactors = F, encoding='UTF-8')
game_co_neg_words <- data.frame(word=t(game_co_neg_words))
rownames(game_co_neg_words)<-c(1:73)
game_co_neg_words <- as.character(game_co_neg_words$word)
```


* 列出隻狼整理好的字典
```{r}
sekiro_lexicon
```


* 使用新的字典和情緒字典斷詞
* 選出 word2 為情緒字的資料
```{r}
# 這裏不加入stop word字典
# 因為清掉的話會影響bigram出來的結果
jieba_tokenizer = worker()

# 使用隻狼字典重新斷詞
# 把否定詞也加入斷詞

new_user_word(jieba_tokenizer, c(sekiro_lexicon,negation_words,game_co_pos_words,game_co_neg_words));

# unnest_tokens 使用的bigram分詞函數
# Input: a character vector
# Output: a list of character vectors of the same length
jieba_bigram <- function(t) {
  lapply(t, function(x) {
    tokens <- segment(x, jieba_tokenizer)
    bigram<- ngrams(unlist(tokens), 2)
    bigram <- lapply(bigram, paste, collapse = " ")
    unlist(bigram)
  })
}


# 執行bigram分詞
sekiro_bigram <- sekiro %>%
  unnest_tokens(bigram, sentence, token = jieba_bigram)


# 將bigram拆成word1和word2
# 將包含英文字母或和數字的詞彙清除
bigrams_separated <- sekiro_bigram %>%
  filter(!str_detect(bigram, regex("[0-9a-zA-Z]"))) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# 並選出word2爲情緒詞的bigram
sekiro_sentiment_bigrams <- bigrams_separated %>%
  filter(!(word1 %in% stop_words), !(word2 %in% stop_words)) %>%
  filter(word2 %in% c(game_co_pos_words, game_co_neg_words))

sekiro_sentiment_bigrams
```



## 畫出原本情緒走勢圖
* 將每一天的情緒走勢畫出
```{r}
# 選出word2中，有出現在情緒詞典中的詞彙
# 如果是正面詞彙則賦予： 情緒標籤爲"positive"、情緒值爲  1
# 如果是負面詞彙則賦予： 情緒標籤爲"negative"、情緒值爲 -1
sekiro_sentiment_bigrams <- sekiro_sentiment_bigrams %>%
  select(artDate, word1, word2) %>%
  mutate(sentiment=ifelse(word2 %in% game_co_pos_words,1,-1), sentiment_tag=ifelse(word2 %in% game_co_pos_words, "positive", "negative"))


# 生成一個時間段中的 日期和情緒標籤的所有可能組合
all_dates <- 
  expand.grid(seq(as.Date(min(sekiro_sentiment_bigrams$artDate)), as.Date(max(sekiro_sentiment_bigrams$artDate)), by="day"), c("positive", "negative"))
names(all_dates) <- c("artDate", "sentiment")

# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- sekiro_sentiment_bigrams %>%
  group_by(artDate,sentiment_tag) %>%
  summarise(count=n())  

# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- all_dates %>% 
  merge(sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(count = replace_na(count, 0))


# 畫圖
sentiment_plot_data %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment))+
  scale_x_date(labels = date_format("%m/%d")) 
```




## 畫出使用 bigram 修正後的情緒走勢圖
* 查看 前面出現否定詞 且 後面爲情緒詞彙 的組合
```{r}
sekiro_sentiment_bigrams %>%
  filter(word1 %in% negation_words) %>%
  count(word1, word2, sort = TRUE) 
```

* bigram 修正後的情緒走勢圖
```{r}
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
sekiro_sentiment_bigrams_negated <- sekiro_sentiment_bigrams %>%
  mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
  mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))

# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- sekiro_sentiment_bigrams_negated %>%
  group_by(artDate,sentiment_tag) %>%
  summarise(count=n())  

# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- all_dates %>% 
  merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(count = replace_na(count, 0))

# 最後把圖畫出來
negated_sentiment_plot_data %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment))+
  scale_x_date(labels = date_format("%m/%d"))
```

### 比較negation 後的情緒走勢
```{r  fig.width=10*4/3, fig.height=10}
# 合併兩種情緒值的資料
all_sentiments <- bind_rows(
  sentiment_plot_data %>% mutate(sentiment=paste(sentiment, "_original", sep = "")),
  negated_sentiment_plot_data %>% mutate(sentiment=paste(sentiment, "_negated", sep = ""))) 


# 先比較正面情緒
all_sentiments %>% 
  filter(sentiment %in% c("positive_original", "positive_negated")) %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment))+
  scale_x_date(labels = date_format("%m/%d"))  -> graph1

# 再比較負面情緒
all_sentiments %>% 
  filter(sentiment %in% c("negative_original", "negative_negated")) %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment))+
  scale_x_date(labels = date_format("%m/%d")) -> graph2

gridExtra::grid.arrange(graph1, graph2, ncol = 1)
```



