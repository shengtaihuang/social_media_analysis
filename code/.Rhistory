library(RCurl)
library(tm)
library(tmcn)
#library(Rwordseg)
require(jiebaR)
library(slam)
library(topicmodels)
library(RWeka)
library(igraph)
library(wordcloud)
library(gbm)
library("Rcpp")
require(randomForest)
require(rpart)
require(rpart.plot)
require(gbm)
#library(Rwordseg)
require(jiebaR)
mixseg=worker(hmm = T)
mixseg=jiebaR::worker(hmm = T)
jiebaR::worker
mixseg=jiebaR::worker(type = "mix",hmm = T)
install.packages("jiebaR")
install.packages("jiebaR")
mixseg=jiebaR::worker(type = "mix",hmm = T)
mixseg=worker(type = "mix",hmm = T)
require(jiebaR)
mixseg=jiebaR::worker(type = "mix",hmm = T)
segment('我的', mixseg )
mixseg=jiebaR::worker(type = "mix")
segment('我的', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
segment('蔡英文', mixseg )
while(1){print(segment('蔡英文', mixseg ))}
mixseg=jiebaR::worker(type = "hmm")
while(1){print(segment('蔡英文', mixseg ))}
segment('甜菜根', mixseg )
segment('張忠謀', mixseg )
while(1)segment('張忠謀', mixseg )
while(1){segment('張忠謀', mixseg )}
while(1){print(segment('張忠謀', mixseg ))}
mixseg=jiebaR::worker(type = "mix")
while(1){print(segment('張忠謀', mixseg ))}
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # ???葉???Ⅳ
packages = c("dplyr", "tidytext", "jiebaR", "gutenbergr", "stringr", "wordcloud2", "ggplot2", "tidyr", "scales")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
require(dplyr)
require(tidytext)
require(jiebaR)
require(gutenbergr)
library(stringr)
library(wordcloud2)
library(ggplot2)
library(tidyr)
library(scales)
# 使用默認參數初始化一個斷詞引擎
jieba_tokenizer = worker()
chi_text <- "孔乙己一到店，所有喝酒的人便都看著他笑，有的叫道，「孔乙己，你臉上又添上新傷疤了！」他不回答，對櫃裡說，「溫兩碗酒，要一碟茴香豆」便排出九文大錢。他們又故意的高聲嚷道，「你一定又偷了人家的東西了！」孔乙己睜大眼睛說，「你怎麼這樣憑空污人清白……」「什麼清白?我前天親眼見你竊了何家的書，吊著打」孔乙己便漲紅了臉，額上的青筋條條綻出，爭辯道，「竊書不能算偷……竊書！……讀書人的事，能算偷麼？」接連便是難懂的話，什麼「君子固窮」，什麼「者乎」之類，引得眾人都鬨笑起來：店內外充滿了快活的空氣。"
# 第一種寫法
segment(chi_text, jieba_tokenizer)
# 第二種寫法
jieba_tokenizer <= chi_text
# 第三種寫法
jieba_tokenizer[chi_text]
chi_text_ptt <- "谷乙己一到店，所有的人便都看着他笑，有的叫道，「谷乙己，你身上又添幾筆新訴訟了！」他不回答，對櫃裡說，「抓兩部片，播幾個視頻」便剪出幾段短片。他們又故意的高聲嚷道，「你一定又用未經授權的片源了！」谷乙己睜大眼睛說，「你怎麼這樣憑空汚人清白……」「什麼清白？我前天親眼見你盜了片商版權，給人吉」谷乙己便漲紅了臉，額上的青筋條條綻出，爭辯道，「二創不能算盜……侵權！……二創的事，能算侵權嗎？」接連便是難懂的話，什麼「創作自由」，什麼「網路著作權」之類，引得衆人都鬨笑起來，店內外充滿了快活的空氣。"
segment(chi_text_ptt, jieba_tokenizer)
# 動態新增自訂詞彙
new_user_word(jieba_tokenizer, c("谷乙己", "未經授權", "汚人清白", "二創","漲紅","臉"))
segment(chi_text_ptt, jieba_tokenizer)
# 使用使用者自訂字典
jieba_tokenizer <- worker(user="user_dict.txt")
segment(chi_text_ptt, jieba_tokenizer)
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
segment(chi_text_ptt, jieba_tokenizer)
# 動態新增停用詞
tokens <- segment(chi_text_ptt, jieba_tokenizer)
stop_words <- c("一到", "幾個", "一定", "能算", "便是")
res <- filter_segment(tokens, stop_words)
# 將詞彙長度為1的詞清除
tokens <- res[nchar(res)>1]
tokens
eng_text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
eng_text
text_df <- tibble(line = 1:4, text = eng_text)
text_df
text_df %>%
unnest_tokens(word, text)
# 初始化斷詞引擎
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
# 自定義斷詞函式
chi_tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
tokens <- tokens[nchar(tokens)>1]
return(tokens)
})
}
chi_tibble <- tibble(line=1:2, text = chi_text_ptt)
chi_tibble %>% unnest_tokens(word, text, token=chi_tokenizer)
chi_text_ptt <- c("谷乙己一到店，所有的人便都看着他笑，有的叫道，「谷乙己，你身上又添幾筆新訴訟了！」他不回答，對櫃裡說，「抓兩部片，播幾個視頻」便剪出幾段短片。他們又故意的高聲嚷道，「你一定又用未經授權的片源了！」谷乙己睜大眼睛說，「你怎麼這樣憑空汚人清白……」「什麼清白？我前天親眼見你盜了片商版權，給人吉」谷乙己便漲紅了臉，額上的青筋條條綻出，爭辯道，「二創不能算盜……侵權！……二創的事，能算侵權嗎？」接連便是難懂的話，什麼「創作自由」，什麼「網路著作權」之類，引得衆人都鬨笑起來，店內外充滿了快活的空氣。", "孔乙己一到店，所有喝酒的人便都看著他笑，有的叫道，「孔乙己，你臉上又添上新傷疤了！」他不回答，對櫃裡說，「溫兩碗酒，要一碟茴香豆」便排出九文大錢。他們又故意的高聲嚷道，「你一定又偷了人家的東西了！」孔乙己睜大眼睛說，「你怎麼這樣憑空污人清白……」「什麼清白?我前天親眼見你竊了何家的書，吊著打」孔乙己便漲紅了臉，額上的青筋條條綻出，爭辯道，「竊書不能算偷……竊書！……讀書人的事，能算偷麼？」接連便是難懂的話，什麼「君子固窮」，什麼「者乎」之類，引得眾人都鬨笑起來：店內外充滿了快活的空氣。")
# 使用 "。" 進行斷句
# 斷詞的依據是使用正規表示式
chi_sentences <- strsplit(chi_text_ptt, "[。]")
chi_sentences
# 使用 jieba_tokenizer 對第一篇文章進行斷詞
chi_sentences[[1]] %>% chi_tokenizer()
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
# 根據上方整理出來的規則，我們可以使用正規表示式，將句子區分章節
red <- red %>%
mutate(chapter = cumsum(str_detect(red$text, regex("^第.*回(　|$)"))))
str(red)
# 下載下來的書已經完成斷句了
head(red, 20)
# 使用紅樓夢專有名詞字典
jieba_tokenizer <- worker(user="dream_of_the_red_chamber_lexicon.traditional.dict", stop_word = "stop_words.txt")
# 設定斷詞function
red_tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
tokens <- red %>% unnest_tokens(word, text, token=red_tokenizer)
str(tokens)
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。.]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．.]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．.]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
rownames(red)
row_number(red)
nrow(red)
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
nrow(red)
nrow(red)
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(24264) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
# 根據上方整理出來的規則，我們可以使用正規表示式，將句子區分章節
red <- red %>%
mutate(chapter = cumsum(str_detect(red$text, regex("^第.*回(　|$)"))))
str(red)
# 下載下來的書已經完成斷句了
head(red, 20)
# 使用紅樓夢專有名詞字典
jieba_tokenizer <- worker(user="dream_of_the_red_chamber_lexicon.traditional.dict", stop_word = "stop_words.txt")
# 設定斷詞function
red_tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
tokens <- red %>% unnest_tokens(word, text, token=red_tokenizer)
str(tokens)
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>10) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(tokens_count, 20)
tokens_count %>% wordcloud2()
plot <-
bind_rows(
red %>%
group_by(chapter) %>%
summarise(count = n(), type="sentences"),
tokens %>%
group_by(chapter) %>%
summarise(count = n(), type="words")) %>%
group_by(type)%>%
ggplot(aes(x = chapter, y=count, fill="type", color=factor(type))) +
geom_line() +
ggtitle("各章節的句子總數") +
xlab("章節") +
ylab("句子數量") #+
#theme(text = element_text(family = "Heiti TC Light"))
plot
frequency <- tokens %>% mutate(part = ifelse(chapter<=80, "First 80", "Last 40")) %>%
filter(nchar(.$word)>1) %>%
mutate(word = str_extract(word, "[^0-9a-z']+")) %>%
mutate(word = str_extract(word, "^[^一二三四五六七八九十]+")) %>%
count(part, word)%>%
group_by(part) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(part, proportion) %>%
gather(part, proportion, `Last 40`)
ggplot(frequency, aes(x = proportion, y = `First 80`, color = abs(`First 80` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
labs(y = "First 80", x = "Last 40")
# 安裝packages
packages = c("readr", "devtools", "stringi", "pbapply", "Rcpp", "RcppProgress")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
# 載入library
library(readr)
library(devtools)
# 解碼scel用
install_github("qinwf/cidian")
library(cidian)
# 簡體轉繁體套件
install_github("qinwf/ropencc")
library(ropencc)
# 載入library
library(readr)
library(devtools)
# 解碼scel用
#install_github("qinwf/cidian")
library(cidian)
# 簡體轉繁體套件
#install_github("qinwf/ropencc")
library(ropencc)
# 解碼scel檔案
decode_scel(scel = "./dream_of_the_red_chamber_lexicon.scel",cpp = TRUE)
# 讀取解碼後生成的詞庫檔案
scan(file="./dream_of_the_red_chamber_lexicon.scel_2019-03-12_17_43_46.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 讀取解碼後生成的詞庫檔案
scan(file="./dream_of_the_red_chamber_lexicon.scel_2019-03-16_15_04_06.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
dict <- read_file("./dream_of_the_red_chamber_lexicon.scel_2019-03-16_15_04_06.dict")
# 將簡體詞庫轉為繁體
cc <- converter(S2TW)
dict_trad <- cc[dict]
write_file(dict_trad, "./dream_of_the_red_chamber_lexicon.traditional.dict")
# 讀取轉換成繁體後的詞庫檔案
scan(file="./dream_of_the_red_chamber_lexicon.traditional.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
plot <-
bind_rows(
red %>%
group_by(chapter) %>%
summarise(count = n(), type="sentences"),
tokens %>%
group_by(chapter) %>%
summarise(count = n(), type="words")) %>%
group_by(type)%>%
ggplot(aes(x = chapter, y=count, fill="type", color=factor(type))) +
geom_line() +
ggtitle("各章節的句子總數") +
xlab("章節") +
ylab("句子數量") #+
#theme(text = element_text(family = "Heiti TC Light"))
plot
frequency <- tokens %>% mutate(part = ifelse(chapter<=80, "First 80", "Last 40")) %>%
filter(nchar(.$word)>1) %>%
mutate(word = str_extract(word, "[^0-9a-z']+")) %>%
mutate(word = str_extract(word, "^[^一二三四五六七八九十]+")) %>%
count(part, word)%>%
group_by(part) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(part, proportion) %>%
gather(part, proportion, `Last 40`)
ggplot(frequency, aes(x = proportion, y = `First 80`, color = abs(`First 80` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
labs(y = "First 80", x = "Last 40")
# 安裝packages
packages = c("readr", "devtools", "stringi", "pbapply", "Rcpp", "RcppProgress")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
# 載入library
library(readr)
library(devtools)
# 解碼scel用
#install_github("qinwf/cidian")
library(cidian)
# 簡體轉繁體套件
#install_github("qinwf/ropencc")
library(ropencc)
# 下載 "紅樓夢" 書籍，並且將text欄位為空的行給清除，以及將重複的語句清除
red <- gutenberg_download(23962) %>% filter(text!="") %>% distinct(gutenberg_id, text)
View(red)
doc = paste0(red$text,collapse = "") #將text欄位的全部文字合併
docVector = unlist(strsplit(doc,"[。．]"), use.names=FALSE) #以全形或半形句號斷句
red = data.frame(gutenberg_id = "23962" , text = docVector,stringsAsFactors = FALSE) #gutenberg_id換成自己的書本id
# 根據上方整理出來的規則，我們可以使用正規表示式，將句子區分章節
red <- red %>%
mutate(chapter = cumsum(str_detect(red$text, regex("^第.*回(　|$)"))))
red$chapter
red$chapter %>% table()
View(red)
red$chapter
# 根據上方整理出來的規則，我們可以使用正規表示式，將句子區分章節
red <- red %>%
mutate(chapter = cumsum(str_detect(red$text, regex("第.*回(　|$)"))))
red$chapter
red$chapter %>% table
# 根據上方整理出來的規則，我們可以使用正規表示式，將句子區分章節
red <- red %>%
mutate(chapter = cumsum(str_detect(red$text, regex("^第.*回(　|$)"))))
# 根據上方整理出來的規則，我們可以使用正規表示式，將句子區分章節
red <- red %>%
mutate(chapter = cumsum(str_detect(red$text, regex("^第.*回"))))
# 使用紅樓夢專有名詞字典
jieba_tokenizer <- worker(user="dream_of_the_red_chamber_lexicon.traditional.dict", stop_word = "stop_words.txt")
# 設定斷詞function
red_tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
# 使用紅樓夢專有名詞字典
jieba_tokenizer <- worker( stop_word = "stop_words.txt")
# 設定斷詞function
red_tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
tokens <- red %>% unnest_tokens(word, text, token=red_tokenizer)
str(tokens)
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>10) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(tokens_count, 20)
tokens_count %>% wordcloud2()
plot <-
bind_rows(
red %>%
group_by(chapter) %>%
summarise(count = n(), type="sentences"),
tokens %>%
group_by(chapter) %>%
summarise(count = n(), type="words")) %>%
group_by(type)%>%
ggplot(aes(x = chapter, y=count, fill="type", color=factor(type))) +
geom_line() +
ggtitle("各章節的句子總數") +
xlab("章節") +
ylab("句子數量") #+
#theme(text = element_text(family = "Heiti TC Light"))
plot
frequency <- tokens %>% mutate(part = ifelse(chapter<=80, "First 80", "Last 40")) %>%
filter(nchar(.$word)>1) %>%
mutate(word = str_extract(word, "[^0-9a-z']+")) %>%
mutate(word = str_extract(word, "^[^一二三四五六七八九十]+")) %>%
count(part, word)%>%
group_by(part) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(part, proportion) %>%
gather(part, proportion, `Last 40`)
ggplot(frequency, aes(x = proportion, y = `First 80`, color = abs(`First 80` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
labs(y = "First 80", x = "Last 40")
# 解碼scel檔案
decode_scel(scel = "west.scel",cpp = TRUE)
setwd("D:/GGA/HW/Text Mining/HW1/code")
# 解碼scel檔案
decode_scel(scel = "west.scel",cpp = TRUE)
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_40.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
dict <- read_file("./west.scel_2019-03-16_15_34_40.dict")
# 將簡體詞庫轉為繁體
cc <- converter(S2TW)
dict_trad <- cc[dict]
write_file(dict_trad, "./dream_of_the_red_chamber_lexicon.traditional.dict")
# 讀取轉換成繁體後的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_40.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_40.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 解碼scel檔案
decode_scel(scel = "west.scel",cpp = TRUE)
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_40.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_40.dict",
what=character(),sep='\n',
encoding='utf-8',fileEncoding='utf-8')
setwd("D:/GGA/HW/Text Mining/HW1/code")
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_28.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 讀取解碼後生成的詞庫檔案
scan(file="./west_2019-03-16_15_34_28.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 讀取解碼後生成的詞庫檔案
scan(file="west_2019-03-16_15_34_28.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
getwd()
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_34_28.dict",
what=character(),sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 解碼scel檔案
decode_scel(scel = "west.scel",cpp = TRUE)
# 解碼scel檔案
decode_scel(scel = "./west.scel",cpp = TRUE)
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_37_44.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
# 安裝packages
packages = c("readr", "devtools", "stringi", "pbapply", "Rcpp", "RcppProgress")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
# 載入library
library(readr)
library(devtools)
# 解碼scel用
#install_github("qinwf/cidian")
library(cidian)
# 簡體轉繁體套件
#install_github("qinwf/ropencc")
library(ropencc)
# 解碼scel檔案
decode_scel(scel = "./west.scel",cpp = TRUE)
# 讀取解碼後生成的詞庫檔案
scan(file="./west.scel_2019-03-16_15_39_39.dict",
what=character(),nlines=50,sep='\n',
encoding='utf-8',fileEncoding='utf-8')
red$chapter %>% table
frequency <- tokens %>% mutate(part = ifelse(chapter<=50, "First 80", "Last 40")) %>%
filter(nchar(.$word)>1) %>%
mutate(word = str_extract(word, "[^0-9a-z']+")) %>%
mutate(word = str_extract(word, "^[^一二三四五六七八九十]+")) %>%
count(part, word)%>%
group_by(part) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(part, proportion) %>%
gather(part, proportion, `Last 40`)
